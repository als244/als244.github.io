<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Andrew Sheinberg">
    <meta name="keywords" content="Andrew Sheinberg">
    <meta name="author" content="Andrew Sheinberg">
    <link rel="stylesheet" href="/static/main.css" type="text/css">
    <title>Andrew Sheinberg</title>

  </head>
  <body>

  <br>

  <h1>Andrew Sheinberg's Homepage</h1>

  <div>
  <a href="/">
  <img src="assets/hiking_utah.jpeg" alt="Andrew Sheinberg" width="400" height="300">
  </a>
  </div>
 
  <br>

  <div>
  <ul>
  <li> <a href="#contact">Contact</a> </li>
  <li> <a href="#aboutMe">About Me</a> </li>
  <li> <a href="#researchInterests">Research Interests</a> </li>
  <li> <a href="favorite_pictures.html">Favorite Pictures</a> </li>
  <li> <a href="travels/travel_home.html">Travels</a> </li>
  <li> <a href="projects/project_home.html">Projects</a> </li>
  </ul>
  </div>

  <hr>
  <div>
    <h2 id="contact"> Contact </h2>
    Feel free to email me at: <a href="mailto:andrew.sheinberg@gmail.com"> andrew.sheinberg@gmail.com </a>
    <br>
    <br>
  </div>

  <hr>
  <div>
    <h2 id="aboutMe"> About Me </h2>

    <p> Hi, I'm Andrew Sheinberg. I'm currently living in Salt Lake City, UT where I've been volunteering remotely with Professor Abhishek Bhattacharjee's group at Yale, reading papers, programming side projects, and exploring nature. Prior to Utah, I lived in Chicago for a around a year where I worked as an options trader on an index volatility strategy desk. Before that, I attended Yale University where I studied Computer Science & Mathematics. I grew up in Barrington, RI where I learned to enjoy athletics and mathletics. Next, I will be applying to graduate school to pursue a Computer Science Ph.D. where I'm hoping to research parallel computing and the memory hierarchy. </p>

    <p> For leisure, I enjoy skiing, surfing, and playing games. I also like taking pictures and have linked some of my favorite photos and memorable travel adventures. </p>
  </div>

  <hr>
  <div>
    <h2 id="researchInterests"> Research Interests </h2>

  <p> I hope to harness the parallelism offered by modern hardware. I would like to dive deeper into the von Neumann memory bottleneck and examine how GPU and multi-core processors can best alleviate this organizational traffic-jam. The diverse memory hierarchy within heterogeneous systems poses challenges for exploiting spatial and temporal locality, and thus is a pain point for reaching maximal performance. </p>

  <p> My starting goal is to better orchestrate data placement and movement in the context of parallel computing. My hunch is that the scratchpad memory capabilities of GPUs' L1 caches are generally being under-utilized, except for in the most hand-tuned kernels. These scratchpad memories highlight an important need to cooperate the partitioning of data and computation. Nvidia's H100 added an additional layer to the programmatic hierarchy with thread block clusters and distributed shared memory; thus, researching locality at a processor scale is an especially rich and timely topic. Locality optimizations have been a fruitful research area dating back to Wilkes' seminal work on caches, and I believe that a new architectural era warrants continued investigation into formalizing and optimizing locality. </p>

  <p> Along with the tightest levels of the memory hierarchy, I am interested in helping optimize locality at a distributed scale, where programs are typically bottlenecked by networking in conjunction with memory. Heterogeneous and multi-GPU environments have become commonplace in a variety of tasks: deep neural network training pretty much everywhere, large scale graph processing at big tech companies and government organizations, physics and chemistry simulations at universities and national labs, high-throughput astronomical data processing at research centers, and many more. A more formal framework of "locality", and thus optimized software, would yield benefits for energy and run-time efficiencies. Moreover, substantial performance improvements may open doors to new heavy-duty computational applications. </p>

  <p> With new generations of programmers being trained alongside big-data and accelerators, I’m optimistic that parallel thinking will become ubiquitous. To foster this widespread paradigm shift in algorithmic thought, I think it’s critical to design clear and principled interfaces that can facilitate maximizing hardware capabilities. Chipmakers have invested heavily in accelerator arms, so within the next decade market pressures will drive prices way down. Thus, we should expect to see widespread usage. We would be better served with an industry-wide interface, and a larger population of proficient parallel programmers. I envision a future where robotic sensors collect orders of magnitude more data than available today. Intelligent, cooperative algorithms will rely on efficient parallel computation, and eventually quantum computation, to process such vast amounts of information. </p>

<u> <h4> My Naive View of History and Possibly Unaddressed Issues </h4> </u>

<p> Memory is today's biggest computational issue: the general purpose von Neumann architectures were not designed to handle massive datasets and parallel computation. Unfortunately, the data-flow models proposed by Dennis, Gurd, and many others in the 60s, 70s, and 80s did not take off. The exponential growth of data availability paired with computationally demanding tasks has revealed the consequences of the industry converging on von Neumann schemes – memory loads are the bottleneck by orders of magnitude. </p>

<p> Enlarging out-of-order execution pipelines has served has a temporary solution to hide latency, but this approach is doomed to hit a wall under realistic physical and budget constraints. In tandem with loading memory out-of-order, we employ a diverse memory hierarchy to optimize where data is loaded from. Wilkes' crafty idea of utilizing caches to exploit spatial and temporal locality, natural computational phenomena, has served the industry well so-far. However, we will continue to face more and more demanding problems. The heuristics current systems use to manipulate cache memory can only work to such an extent – the machine's caching decisions lack "semantic" information about the program's future memory accesses! </p>

<p> Given the state of our current system stack, the task of programming algorithms to effectively utilize hardware caches is quite difficult. It requires first reverse-engineering (reading architecture manual and knowing OS's role) to uncover caching sizes and heuristics. With this information, the programmer can then manipulate the machine (tediously) to their advantage. This is not a feasible solution for widespread efficient-computing. There is the annoyance of portability, obscure data structure layouts, and manual prefetching. A different idea to have fast-code work with the current stack would be to push for cache-oblivious algorithms, which do seem nice in theory. But unfortunately, constants really matter in practice. From an intuitive standpoint, it is hard to imagine cache-oblivious programs reaching maximal practical performance - the program would lack proper "syntactic" information about the machine's capabilities! </p>

<u> <h4> Here & Now </h4> </u>

<p> Improving data movement through hardware-friendly, topology-friendly algorithms will yield gains in energy efficiency and run-time performance. I believe that there is room to more effectively utilize the SRAM caches on GPUs by taking advantage of the scratchpad feature as opposed to the general hardware, heuristic-based L1 policies. Furthermore, there is a new software-managed memory layer in Nvidia's H100 GPU (thread block clusters and distributed shared memory) and this presents an opportunity to modify and improve communication and synchronization patterns. I would like to find an application ripe for optimization and then generalize the methodology to expand the theory behind locality. This is a starting point and hopefully will lead to bolder changes down the line. </p>

<u> <h4> Futuristic Thinking </h4> </u> 
 
<p> A clever compiler and language may be able to bridge the gap. It appears many smart folks are working on this topic – the future is looking bright. This an algorithmic challenge, but moreso a social problem. 

<p> Compiler writers would need to find the right balance of ease of programming, but collect enough semantic information to tell the machine how to do its job properly. With the correct amount of information, hopefully the compiling algorithm can optimize for locality, but a good solution will likely require theoretical underpinnings that I'm not sure exist, yet. I'd like to play a role here :) </p>

</p> The harder challenges lie at a non-technical level, though. Chipmakers would need to cooperate and enable full control of their devices, rather than using proprietary drivers. Further, programmers would need to learn a new interface. Still though, the smartest compiler in the world is only as good as the features offered by the bare-metal. </p>

<p> Recently we have seen an explosion in architectures. Most notably, we now have GPUs with massive parallelism and scratchpad memories. These are features that change the notion of "locality" drastically because computations, accompanied by scratchpad memories, are occurring at different places within the chip and at different temporal frequencies! </p>

<p> Now imagine we are in a distributed setting. We can scale the memory hierarchy upwards to be: disaggregated memory/foreign DRAM to local DRAM to scratchpad memory to registers. I believe the core goal of formalizing locality in order to properly optimize memory still holds and becomes more impactful. </p>

<u> <h4> Thematic Questions </h4> </u>

<p> <ul>
  <li> <b> How does a parallel and distributed environment impact the optimal decisions for data movement and computation scheduling? </b> 
    <ul> <li> What algorithmic changes are necessary to harness the H100's new programmatic layer of thread block clusters and distributed shared memory? </li>
      <li> How will RDMA, processing-in-memory, disaggregated memory, and programmable NICs change this picture? </li>
       <li> Can we model a framework of optimizing spatial (address space/topological) and temporal localities based on certain classes of data structures and/or algorithms?
      <ul> <li> Could this potential framework be the same for various scales of memory hierarchies and topologies? </li> 
      </ul>
         </li>
    </ul> 
  </li>
  <li> <b> Do we take advantage of spatial and temporal localities as well as we can? </b>
             <ul> <li> To what extent has this answer changed in response to newer architectures, such as GPUs and FPGAs? </li>
                  <li> Are the higher levels of the system stack a limiting factor? </li>
             </ul>
        </li>
  <li> <b> Has the absence of locality-optimal thinking from typical programming gone too far? </b>
             <ul> <li> Has it impaired how algorithms are devised? 
      <ul> <li> Should the programmer bear more or different responsibility? </li> </ul>
      </li>
      <li> Is virtual memory sometimes an unnecessary bottleneck (considering RAM capacities now compared to in the 60s)? 
      <ul> <li> What system changes (architecture, OS, compilation, or interface) would be beneficial to best support a programming environment that assumes physically addressed execution? What opportunities are there for improvements under this physically addressed assumption? 
             <ul> 
              <li> Could we make L1 caches larger if they were not restricted by VIPT formatting? </li>                   <li> How much savings from no TLB shootdowns? </li>
             </ul>
           </li>
      </ul>
      </li>
                  <li> If we had a different interface + compiler could we find a natural way for programmers to optimize locality? </li>
             </ul>
        </li>
  <li> <b> Is the fear of parallel programming inflated? </b>
    <ul> <li> If people were exposed to parallel programming earlier on in CS education, would the concepts be easier? (i.e. do people get stuck in their sequential mindset?) </li> 
         <li> How much more efficient would the world's software be if parallel programming paradigm was natural? How many cycles are wasted? </li>
         <li> With more and more data, and hungrier and hungrier algorithms, will/should knowing parallel programming techniques be necessary? </li>
    </ul>
  </li>
    </ul>
</p>

<br>

  </div>

  </body>
</html>
