<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Andrew Sheinberg">
    <meta name="keywords" content="Andrew Sheinberg">
    <meta name="author" content="Andrew Sheinberg">
    <link rel="stylesheet" href="/static/main.css" type="text/css">
    <title>Andrew Sheinberg</title>

  </head>
  <body>

  <br>

  <h1>Andrew Sheinberg's Homepage</h1>

  <div>
  <a href="/">
  <img src="assets/graduation_pro_pic.jpg" alt="Andrew Sheinberg" width="308" height="340">
  </a>
  </div>
 
  <br>

  <div>
  <ul>
  <li> <a href="#contact">Contact</a> </li>
  <li> <a href="#aboutMe">About Me</a> </li>
  <li> <a href="#researchInterests">Research Interests</a> </li>
  <li> <a href="favorite_pictures.html">Favorite Pictures</a> </li>
  <li> <a href="travels/travel_home.html">Travels</a> </li>
  <li> <a href="projects/project_home.html">Projects</a> </li>
  </ul>
  </div>

  <hr>
  <div>
    <h2 id="contact"> Contact </h2>
    Feel free to email me at: <a href="mailto:andrew.sheinberg@gmail.com"> andrew.sheinberg@gmail.com </a>
    <br>
    <br>
  </div>

  <hr>
  <div>
    <h2 id="aboutMe"> About Me </h2>

    <p> Hi, I'm Andrew Sheinberg. I'm currently living in Salt Lake City, UT where I've been volunteering remotely with Professor Abhishek Bhattacharjee's group at Yale, reading papers, programming side projects, and appreciating the best snowfall in recent history by skiing at Alta. Prior to Utah, I lived in Chicago for around a year where I worked as an options trader on an index volatility strategy desk. Before that, I attended Yale University where I studied Computer Science & Mathematics. I grew up in Barrington, RI where I learned to enjoy athletics and mathletics. Next, I will be applying to graduate school to pursue a Computer Science PhD. I plan to research parallel computing and the memory hierarchy, with a focus on exploiting data locality. Pit stops in architecture, operating systems, compilers, and computer networks are to be expected. </p>

    <p> I like taking pictures and have linked some of my favorite photos and memorable travel adventures. </p>
  </div>

  <hr>
  <div>
    <h2 id="researchInterests"> Research Interests </h2>

    <h3> Thematic Questions </h3>

<p> <ul>
  <li> <b> How does a parallel and distributed environment impact the optimal decisions for data placement, data movement, and computation scheduling? </b> 
    <ul> 
        <li> Given assumptions at an architectural level (latency to compute primitive operations and memory hierarchy details) and networking level (topology and latency/bandwidth of the links), <em> can we devise a theoretical model to optimally map classes of computation and communication/synchronization patterns onto distributed hardware? </em>
            <ul>
              <li> How do we deal with the tradeoff between computation cost and communication cost under fixed memory capacity? When to use redundancy? </li>
            </ul>
        </li>
      <li> How will RDMA, processing-in-memory, disaggregated memory, and programmable NICs change this formatting and scheduling picture? </li>
    </ul> 
  </li>
  <li> <b> Do we take advantage of spatial and temporal localities as well as we can? </b>
            <ul> 
              <li> <em> What is the equivalent of Belady's (1966) algorithm in the context of parallel caches and distributed memory? </em> </li> 
              <li> To what extent has this answer changed in response to newer architectures, such as GPUs and FPGAs? 
                  <ul>
                    <li> Are we using the scratchpad memory capabilities of GPUs' L1 caches optimally? (i.e. are we cooperating launching kernels and allocating/assigning memory appropriately?) </li> 
                    <li> What algorithmic changes are necessary to harness the H100's new programmatic layer of thread block clusters and distributed shared memory? </li>
                  </ul>
              </li>
              <li> Are the higher levels of the system stack a limiting factor? </li>
            </ul>
        </li>
  <li> <b> Has the absence of locality-optimal thinking from typical programming gone too far? </b>
      <ul> 
       
        <li> <em> Is virtual memory sometimes an unnecessary bottleneck (considering RAM capacities now compared to in the 60s)? </em>
        <ul> 
          <li> What system changes (architecture, OS, compilation/interface, or networking) would be beneficial to best support a programming environment that assumes physically addressed execution?
            <ul> 
            <li> Are there system design opportunities under this physically addressed assumption? 
             <ul> 
              <li> Could we make L1 caches larger if they were not restricted by VIPT formatting? </li>                   
              <li> How much savings from no TLB shootdowns? </li>
              <li> Would physically addressed execution simplify OS/networking stacks, such that it would enhance parallel programs at a distributed scale? </li>
             </ul>
            </li>
            </ul>
           </li>
        </ul>
      </li>
       <li> Has it impaired how algorithms are devised? 
            <ul> 
              <li> Should the programmer bear more or different responsibility? </li> 
              <li> If we had a different interface + compiler could we find a natural way for programmers to optimize locality? </li>
            </ul>
        </li>
      
      </ul>
  </li>
  <li> <b> Is the fear of parallel programming inflated? </b>
    <ul> <li> <em> With more and more data, and hungrier and hungrier algorithms, should/will knowing parallel programming techniques be necessary? </em> </li>
          <li> If people were exposed to parallel programming earlier on in CS education, would the concepts be easier? (i.e. do people get stuck in their sequential mindset?) </li> 
          <ul> <li> Does having a small circle of expert developers publish parallel computing libraries (e.g. cuBLAS, cuDNN => PyTorch, TensorFlow, and the like) have long-term negative externalities for effective programming practice? </li>
                <li> Don't we want people to understand how the machines operate (i.e. what software is <em> fully </em> doing) and not just "guess"...? </li>
          </ul>
         <li> How much more efficient would the world's software be if parallel programming paradigm was natural? How many cycles are wasted? </li>
    </ul>
  </li>
  <li> <b> Would an effective data-flow processor, and system stack on top, render all these questions moot? </b> </li>
    </ul>
</p>

 <br>


<u> <h3> My Naive View of History and Possibly Unaddressed Issues </h3> </u>

<p> Memory is today's biggest computational issue: the general purpose von Neumann architectures were not designed to handle massive datasets and parallel computation. Unfortunately, the data-flow models proposed by Petri (1962), Karp and Miller (1966), Kahn (1974), Dennis (1974), Gurd and Watson (1977), Avrind (1981) and many others in the 60s, 70s, and 80s did not take off. The exponential growth of data availability paired with computationally demanding tasks has revealed the consequences of the industry converging on von Neumann schemes – loading data is the clear bottleneck.
  
<p> Enlarging out-of-order execution pipelines has served has a temporary solution to hide latency, but this approach is doomed to hit a wall under realistic physical and budget constraints. In tandem with loading memory out-of-order, we employ a diverse memory hierarchy to optimize where data is loaded from. Wilkes' crafty idea of utilizing caches was a gamechanger. Caches are used to exploit temporal (Wilkes 1965) and spatial (Liptay 1968) localities, seemingly natural computational phenomena. They have served the industry well so-far; caches have worked to such an extent that the industry has just grown the memory hierarchy with more of them to boost performance - L1, L2, L3! However, we will continue to face more and more demanding problems. The heuristics current systems use to manipulate cache memory can only work to such an extent – the machine's caching decisions lack "semantic" information about the program's future memory accesses! </p>

<p> Given the state of our current system stack, the task of programming algorithms to effectively utilize hardware caches is quite difficult. It requires first reverse-engineering (reading architecture manual and knowing OS's role) to uncover caching sizes and heuristics. With this information, the programmer can then manipulate the machine (tediously) to their advantage. This is not a feasible solution for widespread efficient-computing. There is the annoyance of portability, obscure data structure layouts, and manual prefetching. A different idea to have fast-code work with the current stack would be to push for cache-oblivious algorithms, which do seem nice in theory. But unfortunately, constants really matter in practice. From an intuitive standpoint, it is hard to imagine cache-oblivious programs reaching maximal practical performance - the program would lack proper "syntactic" information about the machine's capabilities! </p>

<u> <h3> Here & Now </h3> </u>

<p> Improving data movement through hardware-friendly, topology-friendly algorithms will yield gains in energy efficiency and run-time performance. I believe that there is room to more effectively utilize the SRAM caches on GPUs by taking advantage of the scratchpad feature as opposed to the general hardware, heuristic-based L1 policies. Furthermore, there is a new software-managed memory layer in Nvidia's H100 GPU (thread block clusters and distributed shared memory) and this presents an opportunity to modify and improve communication and synchronization patterns. I would like to find an application ripe for optimization and then generalize the methodology to expand the theory behind locality. This is a starting point and hopefully will lead to bolder changes down the line. </p>

<u> <h3> Futuristic Thinking </h3> </u> 
 
<p> A clever compiler and language may be able to bridge the gap. It appears many smart folks are working on this topic – the future is looking bright. This an algorithmic challenge, but moreso a social problem. </p>

<p> Compiler writers would need to find the right balance of ease of programming, but collect enough semantic information to tell the machine how to do its job properly. With the correct amount of information, hopefully the compiling algorithm can optimize for locality, but a good solution will likely require theoretical underpinnings that I'm not sure exist, yet. I imagine playing a role here. </p>

</p> The harder challenges lie at a non-technical level, though. Chipmakers would ideally cooperate to enable full control of their GPUs, rather than forcing machines to use proprietary drivers. Further, programmers would need to learn a new interface. Still though, the smartest compiler in the world is only as good as the features offered by the bare-metal. </p>

<p> Recently we have seen an explosion in architectures. Most notably, we now have GPUs with massive parallelism and scratchpad memories. These are features that change the notion of "locality" drastically because computations, accompanied by scratchpad memories, are occurring at different places within the chip and at different temporal frequencies! </p>

<p> Now imagine we are in a distributed setting. We can scale the memory hierarchy upwards to be: disaggregated memory/foreign DRAM to local DRAM to scratchpad memory to registers. I believe the core goal of formalizing locality in order to properly optimize memory still holds and becomes more impactful. </p>


<br>

  </div>

  </body>
</html>
